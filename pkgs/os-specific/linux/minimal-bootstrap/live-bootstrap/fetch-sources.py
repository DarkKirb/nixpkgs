#! /usr/bin/env nix-shell
#! nix-shell -i python3 -p python3 nix

# Generate a sources.nix based on an input nix file
# It will either fill in missing hashes or fully re-fetch hashes (regen)
# You can safely output to the same file you've input as it's read before rewriting
#
# If you don't provide an input (`--input <INPUT_FILE>`) the script will look next to itself
#
# The input nix file should be in the format:
# {
#   "<COMMIT>" {
#     "<FILE_PATH>": "<HASH>";
#     "<FILE_PATH>": "";
#   };
#   "<COMMIT>" {
#     "<FILE_PATH>": "<HASH>";
#     "<FILE_PATH>": "";
#   };
# }
#
# Examples:
#   ./fetch-sources.nix > sources.nix
#   ./fetch-sources.nix -i otherfile.nix > sources.nix
#   ./fetch-sources.nix --refresh > sources.nix
# See also --help

import argparse
import concurrent.futures
import json
import os
from functools import partial
import sys

ERR = "FAILED TO FETCH"

def log(*args, **kwargs):
  print(*args, file=sys.stderr, **kwargs)

def fetch_file(commit, filepath):
  url = "https://github.com/fosslinux/live-bootstrap/raw/{0}/{1}".format(commit, filepath)
  name = "source"
  # keep inline with default.nix so they're found in the store rather than re-fetched
  # note: doesn't affect getting the right sha
  # e.g. /nix/store/3425qxmzdw3q3yarx4jcqcslair5dvd5-live-bootstrap-1bc4296-sysa-gzip-1.2.4-mk-main.mk
  name = "{pname}-${short_commit}-{file}".format(
    pname="live-bootstrap",
    # get all letters to n-1
    short_commit=commit[:8],
    file=filepath.replace("/", "-")
  )
  prefix = "{0}@{1}:".format(commit, filepath)
  log(prefix, "fetching")
  stream = os.popen("nix-prefetch-url {0} --type sha256 --name {1}".format(url, name))
  # strip trailing newline
  output = stream.read().strip()
  if len(output) == 0:
    log(prefix, "failed, setting to", ERR)
    return filepath, ERR
  log(prefix, "done")
  return filepath, output

def return_empty(dictionary):
  return {k: v for k, v in dictionary.items() if len(v) == 0}

def fetch_files_for_commit(regen, source_dict):
  commit, files_dict = source_dict
  result = {}
  # pre-feed function the commit
  ff = partial(fetch_file, commit)
  # if we're not doing a full regeneration filter out just the files we need to fetch
  to_process = files_dict if regen else return_empty(files_dict)
  # concurrently fetch hashes for all files
  with concurrent.futures.ProcessPoolExecutor() as executor:
    for key, new_value in executor.map(ff, to_process.keys()):
      # add results to our result dict
      result[key] = new_value
  return commit, result

def fetch_sources(regen, sources):
  result = {}
  # pre-feed function regen
  fffc = partial(fetch_files_for_commit, regen)
  with concurrent.futures.ProcessPoolExecutor() as executor:
    for key, new_value in executor.map(fffc, sources.items()):
      result[key] = new_value
  return result

def update_sources(old, new):
  merged = {}
  for commit, sources in new.items():
    for filename, hash_value in sources.items():
      old[commit][filename] = hash_value
  return old

def process_to_nix(full):
  string = """# This file is generated by ./fetch-sources.py.
# Do not edit apart from in the ways outlined below:
#
# `./fetch-sources.py <INPUT_FILE>` writes to STDOUT but you can
# safely output over the existing sources.nix
#
# To add a new file:
# 1. Put the filepath and an empty hash
# 2. Run the script and it will populate that hash
# 3. If a file does not exist on that commit the hash will be set to
#    "{0}" which will break fetching to make the issue known
#
# To re-fetch/refresh all hashes:
# 1. Run `./fetch-sources.sh <INPUT_FILE> --regen > <OUTPUT_FILE>`
#
# !!! Any other changes will not be persisted to the output !!!
{{
""".format(ERR)
  for commit, sources in full.items():
    string += '  "{0}" = {{\n'.format(commit)
    for filename, hash_value in sources.items():
      string += '    "{0}" = "{1}";\n'.format(filename, hash_value)
    string += "  };\n"
  # print adds newline
  string += "}"
  return string

def order_dict(dictionary):
  return {k: order_dict(v) if isinstance(v, dict) else v
      for k, v in sorted(dictionary.items())}


def main(args):
  script_dir = os.path.dirname(os.path.realpath(__file__))
  input_file = args.input or script_dir + "/sources.nix"
  stream = os.popen("nix-instantiate --eval --strict {0} --json".format(input_file))
  output = stream.read()

  output_dict = json.loads(output)
  regen = args.regen
  if regen:
    log("updating all hashes")
  else:
    log("updating only missing hashes")
  # start concurrently fetching sources
  new_sources = fetch_sources(regen, output_dict)
  # squash together the old sources with the updates
  merged_sources = update_sources(output_dict, new_sources)
  # empty log to stderr to split stderr from stdout, makes it a bit more readable
  log()
  # format our ordered resulting sources
  output = process_to_nix(order_dict(merged_sources))
  print()
  script_dir
  if args.overwrite:
    f = open(script_dir + "/sources.nix", "w")
    # write and account for missing the free newline from print
    f.write(output + "\n")
    f.close()
  else:
    print(output)


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument("-r", "--regen", help="Regenerate all hashes", action=argparse.BooleanOptionalAction)
  parser.add_argument("-o", "--overwrite", help="Overwrite sources.nix relative to the script", action=argparse.BooleanOptionalAction)
  parser.add_argument("-i", "--input", help="Input sources.nix", nargs='?', type=str)
  args = parser.parse_args()
  main(args)
